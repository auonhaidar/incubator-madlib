# coding=utf-8

"""
@file perceptron.py_in

@namespace perceptron

@brief Perceptron: Driver functions
"""
import plpy
from utilities.group_control import GroupIterationController
from utilities.utilities import unique_string
from utilities.validate_args import table_exists
from utilities.validate_args import columns_exist_in_table
from utilities.validate_args import table_is_empty
from utilities.validate_args import explicit_bool_to_text
from utilities.utilities import _string_to_array
from utilities.utilities import __mad_version
from utilities.utilities import add_postfix
from utilities.utilities import _string_to_array_with_quotes

from collections import defaultdict

version_wrapper = __mad_version()
# ========================================================================


def __compute_perceptron(schema_madlib, rel_args, rel_state, rel_source, dep_col, ind_col, optimizer, col_grp_iteration, col_grp_state, **kwargs):
    
    #optimizer = "igd"
    iterationCtrl = GroupIterationController(
        rel_args=rel_args,
        rel_state=rel_state,
        stateType="double precision[]",
        # truncAfterIteration=False,
        schema_madlib=schema_madlib,  # Identifiers start here
        rel_source=rel_source,
        ind_col=ind_col,
        dep_col=dep_col,
        optimizer=optimizer,
        grouping_col=None,
        grouping_str="Null",
        col_grp_iteration=col_grp_iteration,
        col_grp_state=col_grp_state)

    if optimizer == "igd":
        with iterationCtrl as it:
            it.iteration = 0
            while True:
                it.update(
                    """
                    {schema_madlib}.__perceptron_{optimizer}_step(
                        ({dep_col})::boolean,
                        ({ind_col})::double precision[],
                        rel_state.{col_grp_state})
                    """)
                if it.test(
                        """
                        {iteration} >= _args.max_iter
                        or
                        _state_current[array_upper(_state_current, 1)] = 3
                        or
                        {schema_madlib}.__perceptron_{optimizer}_step_distance(
                            _state_previous, _state_current) < _args.tolerance
                        """):
                    break
    if optimizer == "sigmoid":
        with iterationCtrl as it:
            it.iteration = 0
            while True:
                it.update(
                    """
                    {schema_madlib}.__sigmoid_igd_step(
                        ({dep_col})::boolean,
                        ({ind_col})::double precision[],
                        rel_state.{col_grp_state})
                    """)
                if it.test(
                        """
                        {iteration} >= _args.max_iter
                        or
                        _state_current[array_upper(_state_current, 1)] = 3
                        or
                        {schema_madlib}.__sigmoid_igd_step_distance(
                            _state_previous, _state_current) < _args.tolerance
                        """):
                    break
    if optimizer == "tanh":
        with iterationCtrl as it:
            it.iteration = 0
            while True:
                it.update(
                    """
                    {schema_madlib}.__tanh_igd_step(
                        ({dep_col})::boolean,
                        ({ind_col})::double precision[],
                        rel_state.{col_grp_state})
                    """)
                if it.test(
                        """
                        {iteration} >= _args.max_iter
                        or
                        _state_current[array_upper(_state_current, 1)] = 3
                        or
                        {schema_madlib}.__tanh_igd_step_distance(
                            _state_previous, _state_current) < _args.tolerance
                        """):
                    break
    return iterationCtrl.iteration

# ========================================================================


def perceptron_train(schema_madlib, source_table, out_table, dependent_varname, independent_varname, errorfn, actfn, max_iter, tolerance, **kwargs):
    #tolerance = 0.000001
    grouping_cols=None
    optimizer = actfn
    if errorfn == "loglik":
        optimizer="igd"

    
    verbose=None
    __perceptron_validate_args(
        schema_madlib, source_table, out_table, dependent_varname,
        independent_varname, optimizer, max_iter, tolerance)

    return __perceptron_train_compute(
        schema_madlib, source_table, out_table, dependent_varname,
        independent_varname, optimizer, max_iter, tolerance, **kwargs)

# ========================================================================


def __perceptron_validate_args(schema_madlib, tbl_source, tbl_output, dep_col, ind_col, optimizer, max_iter, tolerance):
    """
    Validate the arguments
    """
    if tbl_source is None or tbl_source.strip().lower() in ('null', ''):
        plpy.error("perceptron error: Invalid data table name!")
    if not table_exists(tbl_source):
        plpy.error("perceptron error: Data table does not exist!")
    if table_is_empty(tbl_source):
        plpy.error("perceptron error: Data table is empty!")

    if tbl_output is None or tbl_output.strip().lower() in ('null', ''):
        plpy.error("perceptron error: Invalid output table name!")

    if (table_exists(tbl_output, only_first_schema=True)):
        plpy.error("Output table name already exists. Drop the table before calling the function.")

    if not dep_col or dep_col.strip().lower() in ('null', ''):
        plpy.error("perceptron error: Invalid dependent column name!")

    if optimizer not in ("igd", "sigmoid", "tanh"):
        plpy.error("perceptron error: Invalid activation function!")        

    # if not columns_exist_in_table(tbl_source, [dep_col]):
    #     plpy.error("perceptron error: Dependent column does not exist!")

    if not ind_col or ind_col.lower() in ('null', ''):
        plpy.error("perceptron error: Invalid independent column name!")


    if max_iter <= 0:
        plpy.error("perceptron error: Maximum number of iterations must be positive!")

    if tolerance < 0:
        plpy.error("perceptron error: The tolerance cannot be negative!")

# ========================================================================


def __perceptron_train_compute(schema_madlib, tbl_source, tbl_output, dep_col, ind_col, optimizer, max_iter, tolerance, **kwargs):
    """
    Create an output table (drop if exists) that contains the perceptron model
    """
    old_msg_level = plpy.execute("select setting from pg_settings where \
                                  name='client_min_messages'")[0]['setting']
    verbose = True
    if verbose:
        plpy.execute("set client_min_messages to warning")
    else:
        plpy.execute("set client_min_messages to error")
    grouping_col = None
    grouping_str = "Null"
    #optimizer = "igd"   
    args = {'schema_madlib': schema_madlib,
            'tbl_source': tbl_source,
            'tbl_output': tbl_output,
            'dep_col': dep_col,
            'ind_col': ind_col,
            'max_iter': max_iter,
            'tolerance': tolerance,
            'optimizer': optimizer,
            'tbl_perceptron_args': unique_string(),
            'tbl_perceptron_state': unique_string(),
            'col_grp_iteration': unique_string(),
            'col_grp_state': unique_string(),
            'igd': "__perceptron_igd_result",
            'sigmoid': "__sigmoid_igd_result",
            'tanh': "__tanh_igd_result"}

    plpy.execute("select {schema_madlib}.create_schema_pg_temp()".format(**args))
    plpy.execute(
        """
        drop table if exists pg_temp.{tbl_perceptron_args};
        create table pg_temp.{tbl_perceptron_args} as
            select
                {max_iter} as max_iter,
                {tolerance} as tolerance
        """.format(**args))

    # return an array of dict
    # each dict has two elements: iteration number, and grouping value array
    
    iteration_run = __compute_perceptron(schema_madlib, args["tbl_perceptron_args"],
                                      args["tbl_perceptron_state"], tbl_source,
                                      dep_col, ind_col, optimizer,
                                      col_grp_iteration=args["col_grp_iteration"],
                                      col_grp_state=args["col_grp_state"])

    grouping_str1 = "" if grouping_col is None else grouping_col + ","
    grouping_str2 = "1 = 1" if grouping_col is None else grouping_col
    using_str = "" if grouping_str1 == "" else "using (" + grouping_col + ")"
    join_str = "," if grouping_str1 == "" else "join "

    if optimizer == "igd":
        plpy.execute(
            """
            drop table if exists {tbl_output};
            create table {tbl_output} as
                select
                    {grouping_str1}
                    (case when (result).status = 1 then (result).coef
                        else NULL::double precision[] end) as weights,
                    (case when (result).status = 1 then (result).log_likelihood
                        else NULL::double precision end) as log_likelihood,
                    {col_grp_iteration} as num_iterations
                from
                (
                    select
                        {col_grp_iteration}, {grouping_str1} result, num_rows
                    from
                    (
                        (select
                            {grouping_str1}
                            {schema_madlib}.{fnName}({col_grp_state}) as result,
                            {col_grp_iteration}
                        from
                            {tbl_perceptron_state}
                        ) t
                        join
                        (
                        select
                            {grouping_str1}
                            max({col_grp_iteration}) as {col_grp_iteration}
                        from {tbl_perceptron_state}
                        group by {grouping_str2}
                        ) s
                        using ({grouping_str1} {col_grp_iteration})
                    ) q1
                    {join_str}
                    (
                        select
                            {grouping_str1}
                            count(*) num_rows
                        from {tbl_source}
                        group by {grouping_str2}
                    ) q2
                    {using_str}
                ) q3
            """.format(grouping_str1=grouping_str1,
                       grouping_str2=grouping_str2,
                       fnName=args[args["optimizer"]],
                       iteration_run=iteration_run,
                       using_str=using_str,
                       join_str=join_str,
                       **args))
    else:
        plpy.execute(
            """
            drop table if exists {tbl_output};
            create table {tbl_output} as
                select
                    {grouping_str1}
                    (case when (result).status = 1 then (result).coef
                        else NULL::double precision[] end) as weights,
                    (case when (result).status = 1 then (result).log_likelihood
                        else NULL::double precision end) as rmse_error,
                    {col_grp_iteration} as num_iterations
                from
                (
                    select
                        {col_grp_iteration}, {grouping_str1} result, num_rows
                    from
                    (
                        (select
                            {grouping_str1}
                            {schema_madlib}.{fnName}({col_grp_state}) as result,
                            {col_grp_iteration}
                        from
                            {tbl_perceptron_state}
                        ) t
                        join
                        (
                        select
                            {grouping_str1}
                            max({col_grp_iteration}) as {col_grp_iteration}
                        from {tbl_perceptron_state}
                        group by {grouping_str2}
                        ) s
                        using ({grouping_str1} {col_grp_iteration})
                    ) q1
                    {join_str}
                    (
                        select
                            {grouping_str1}
                            count(*) num_rows
                        from {tbl_source}
                        group by {grouping_str2}
                    ) q2
                    {using_str}
                ) q3
            """.format(grouping_str1=grouping_str1,
                       grouping_str2=grouping_str2,
                       fnName=args[args["optimizer"]],
                       iteration_run=iteration_run,
                       using_str=using_str,
                       join_str=join_str,
                       **args))
    
    tbl_output_summary = add_postfix(tbl_output, "_summary")
    if optimizer == "igd":
        plpy.execute(
            """
            create table {tbl_output_summary} as
                select
                    'Log_likelihood'::varchar           as error_function,
                    '{tbl_source}'::varchar             as source_table,
                    '{tbl_output}'::varchar             as out_table,
                    '{dep_col}'::varchar                as dependent_varname,
                    '{ind_col}'::varchar                as independent_varname,
                    'activation_fn=sigmoid, max_iter={max_iter}, tolerance={tolerance}'::varchar as optimizer_params
                   
                   
                    
            """.format(tbl_output_summary=tbl_output_summary, **args))

    if optimizer == "sigmoid":
        plpy.execute(
            """
            create table {tbl_output_summary} as
                select
                    'RMSE'::varchar                     as error_function,
                    '{tbl_source}'::varchar             as source_table,
                    '{tbl_output}'::varchar             as out_table,
                    '{dep_col}'::varchar                as dependent_varname,
                    '{ind_col}'::varchar                as independent_varname,
                    'activation_fn=sigmoid, max_iter={max_iter}, tolerance={tolerance}'::varchar as optimizer_params
                   
                   
                    
            """.format(tbl_output_summary=tbl_output_summary, **args))
    if optimizer == "tanh":
        plpy.execute(
            """
            create table {tbl_output_summary} as
                select
                    'RMSE'::varchar                     as error_function,
                    '{tbl_source}'::varchar             as source_table,
                    '{tbl_output}'::varchar             as out_table,
                    '{dep_col}'::varchar                as dependent_varname,
                    '{ind_col}'::varchar                as independent_varname,
                    'activation_fn=tan_hyperbolic, max_iter={max_iter}, tolerance={tolerance}'::varchar as optimizer_params
                   
                   
                    
            """.format(tbl_output_summary=tbl_output_summary, **args))
    plpy.execute("""
                 drop table if exists pg_temp.{tbl_perceptron_args};
                 drop table if exists pg_temp.{tbl_perceptron_state}
                 """.format(**args))

    plpy.execute("set client_min_messages to " + old_msg_level)
    return None
# --------------------------------------------------------------------


def perceptron_help_msg(schema_madlib, message, **kwargs):
    """ Help message for Perceptron

    @param message A string, the help message indicator

    Returns:
      A string, contains the help message
    """
    if not message:

        help_string = """
----------------------------------------------------------------
                        SUMMARY
----------------------------------------------------------------
Perceptron models the relationship between a
dichotomous dependent variable and one or more predictor variables.

The dependent variable may be a Boolean value or a categorical variable
that can be represented with a Boolean expression.

For more details on function usage:
    SELECT {schema_madlib}.perceptron_train('usage')

For a small example on using the function:
    SELECT {schema_madlib}.perceptron_train('example')
        """
    elif message in ['usage', 'help', '?']:

        help_string = """
------------------------------------------------------------------
                        USAGE
------------------------------------------------------------------
SELECT {schema_madlib}.perceptron_train(
    source_table,         -- name of input table
    out_table,            -- name of output table
    dependent_varname,    -- name of dependent variable
    independent_varname,  -- names of independent variables
    error_function,       -- two options, loglik or rmse
    actfn,                -- Activation Function, sigmoid or tanh
    max_iter,             -- optional, default 20, maximum iteration number  
    tolerance,            -- optional, default 0.0001, the stopping threshold
);

------------------------------------------------------------------
                        OUTPUT
------------------------------------------------------------------
The output table ('out_table' above) has the following columns:
    'weights',                     double precision[], -- vector of learnt weights
    'log_likelihood/RMSE',         double precision,   -- log likelihood or RMSE for training set
    'num_iterations'               double precision    -- Maximum number of iterations of the perceptron algorithm that the user wants to run before stopping it
    

A summary table named <out_table>_summary is also created:
    'error_function'            varchar,    -- error function name ('Log_likelihood' or 'RMSE')
    'source_table'              varchar,    -- the data source table name
    'out_table'                 varchar,    -- the output table name
    'dependent_varname'         varchar,    -- the dependent variable
    'independent_varname'       varchar,    -- the independent variable
    'optimizer_params'          varchar,    -- 'activation_fn=..., max_iter=..., tolerance=...'
    """
    elif message in ['example', 'examples']:

        help_string = """
CREATE TABLE andgate( x INTEGER NOT NULL,
                       y INTEGER NOT NULL,
                       label INTEGER);
COPY andgate FROM STDIN WITH DELIMITER '|';
   1 | 1 | 1
 1 | 0 | 0
 0 | 1 | 0
 0 | 0 | 0
 1 | 1 | 1
 1 | 1 | 1
 1 | 1 | 1
 1 | 0 | 0
 0 | 0 | 0
 0 | 0 | 0
 0 | 0 | 0
 0 | 1 | 0
 1 | 1 | 0
 1 | 1 | 0
 1 | 1 | 1
 1 | 1 | 1
 1 | 1 | 1
 1 | 1 | 1
 1 | 1 | 1
 1 | 1 | 1
 1 | 1 | 1
 1 | 1 | 1
 1 | 1 | 1
 1 | 1 | 1
 1 | 1 | 1
\.

-- Drop output tables before calling the function
DROP TABLE IF EXISTS model;
DROP TABLE IF EXISTS model_summary;

SELECT madlib.perceptron_train( 'andgate',
                             'model',
                             'label',
                             'ARRAY[1, x, y]', 'loglik', 'sigmoid'
                           );

SELECT * from model;
        """
    else:
        help_string = "No such option. Use {schema_madlib}.perceptron_train('help')"

    return help_string.format(schema_madlib=schema_madlib)
## ========================================================================


def perceptron_predict_help(schema_madlib, message, **kwargs):
    
    if not message:
        message = "summary"
    elif message.lower() in ['usage', 'help', '?']:
        message = 'usage'
    elif message.lower() in ['example', 'examples']:
        message = 'example'
    else:
        message = 'unknown'

    help_string = defaultdict(str)
    help_string['summary'] = """
----------------------------------------------------------------
                        SUMMARY
----------------------------------------------------------------
Prediction in Perceptron is done my Perceptron predict function.

        """

    help_string['usage'] = """
------------------------------------------------------------------
                        USAGE
------------------------------------------------------------------
perceptron_predict(
    weights,        -- DOUBLE PRECISION[], Learnt weights by Perceptron
    col_ind_var  -- DOUBLE PRECISION[], Values for the independent variables
)

The lengths of 'weight' array and 'col_ind_var' array above should be equal.
------------------------------------------------------------------
                        OUTPUT
------------------------------------------------------------------
The output of the function is a BOOLEAN value representing the predicted
dependent variable value for the inputed independent variables.
        """

    help_string['example'] = """

        """

    help_string['unknown'] = "No such option. Use {schema_madlib}.perceptron_predict()"

    return help_string[message].format(schema_madlib=schema_madlib)
# -------------------------------------------------------------------------


def perceptron_predict_prob_help(schema_madlib, message, **kwargs):
    
    if not message or message == 'summary':
        message = "summary"
    elif message.lower() in ['usage', 'help', '?']:
        message = 'usage'
    elif message.lower() in ['example', 'examples']:
        message = 'example'
    else:
        message = 'unknown'

    help_string = defaultdict(str)
    help_string['summary'] = """
----------------------------------------------------------------
                        SUMMARY
----------------------------------------------------------------
This function can be used to obtain a prediction of the probability of
the dependent variable in Perceptron given the value of
independent variables.
"""

    help_string['usage'] = """
------------------------------------------------------------------
                        USAGE
------------------------------------------------------------------
perceptron_predict_prob(
    weights,        -- DOUBLE PRECISION[], Learnt weights of Perceptron
    col_ind_var  -- DOUBLE PRECISION[], Values for the independent variables
)

Following method should be used for tanh activation function:

perceptron_predict_prob_tanh(
    weights,        -- DOUBLE PRECISION[], Learnt weights of Perceptron
    col_ind_var  -- DOUBLE PRECISION[], Values for the independent variables
)
The lengths of 'weights' array and 'col_ind_var' array above should be equal.
------------------------------------------------------------------
                        OUTPUT
------------------------------------------------------------------
The output of the function is a DOUBLE PRECISION value representing the
probability of predicted dependent variable value being TRUE for the inputed
independent variable values.
        """

    help_string['example'] = """
        """

    help_string['unknown'] = "No such option. Use {schema_madlib}.perceptron_predict_prob()"

    return help_string[message].format(schema_madlib=schema_madlib)
